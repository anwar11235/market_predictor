{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Predictor: Feature Engineering\n",
    "\n",
    "This notebook focuses on generating and analyzing features from our collected data:\n",
    "1. Technical Features\n",
    "2. Sentiment Features\n",
    "3. Macroeconomic Features\n",
    "4. Feature Selection and Analysis\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.features import (\n",
    "    TechnicalFeatures,\n",
    "    SentimentFeatures,\n",
    "    MacroFeatures,\n",
    "    FeatureGenerator\n",
    ")\n",
    "from src.utils import setup_project_logger\n",
    "from config import Config, load_validated_config\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_project_logger('feature_engineering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Configuration\n",
    "\n",
    "Load the processed data from our previous notebook and configure feature parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_validated_config('config/parameters.yaml')\n",
    "\n",
    "# Load processed data\n",
    "market_data = pd.read_parquet('data/processed/market_data.parquet')\n",
    "macro_data = pd.read_parquet('data/processed/macro_data.parquet')\n",
    "\n",
    "# Load news and social data if available\n",
    "news_data = {}\n",
    "social_data = {}\n",
    "\n",
    "try:\n",
    "    for source in ['newsapi', 'alphavantage', 'finnhub']:\n",
    "        file_path = f'data/processed/news_{source}.parquet'\n",
    "        if os.path.exists(file_path):\n",
    "            news_data[source] = pd.read_parquet(file_path)\n",
    "            \n",
    "    for source in ['reddit', 'twitter']:\n",
    "        file_path = f'data/processed/social_{source}.parquet'\n",
    "        if os.path.exists(file_path):\n",
    "            social_data[source] = pd.read_parquet(file_path)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Error loading some data sources: {e}\")\n",
    "\n",
    "# Display data information\n",
    "print(\"Data Ranges:\")\n",
    "print(f\"Market Data: {market_data.index.min()} to {market_data.index.max()}\")\n",
    "print(f\"Macro Data: {macro_data.index.min()} to {macro_data.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Technical Feature Generation\n",
    "\n",
    "Generate technical indicators and analyze their distributions and relationships:\n",
    "- Price-based features\n",
    "- Volume indicators\n",
    "- Momentum indicators\n",
    "- Volatility measures\n",
    "- Pattern recognition features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize technical feature generator\n",
    "tech_features = TechnicalFeatures(config)\n",
    "\n",
    "# Generate technical features\n",
    "technical_features = tech_features.calculate_all_features(market_data)\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\nTechnical Features Overview:\")\n",
    "print(technical_features.info())\n",
    "\n",
    "# Plot key feature distributions\n",
    "key_features = [\n",
    "    'Returns', 'Daily_Volatility',\n",
    "    'RSI_14', 'MFI',\n",
    "    'ATR', 'OBV'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    if feature in technical_features.columns:\n",
    "        sns.histplot(data=technical_features, x=feature, ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Calculate feature correlations\n",
    "correlation_matrix = technical_features[key_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f')\n",
    "plt.title('Technical Feature Correlations')\n",
    "plt.show()\n",
    "\n",
    "# Log feature generation completion\n",
    "logger.info(f\"Generated {len(technical_features.columns)} technical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment Feature Generation\n",
    "\n",
    "Process and combine sentiment data from multiple sources:\n",
    "- News sentiment analysis\n",
    "- Social media sentiment\n",
    "- Aggregated sentiment indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize sentiment feature generator\n",
    "sentiment_features = SentimentFeatures(config)\n",
    "\n",
    "# Generate sentiment features from news data\n",
    "news_features = pd.DataFrame(index=market_data.index)\n",
    "\n",
    "for source, data in news_data.items():\n",
    "    if not data.empty:\n",
    "        source_features = sentiment_features.calculate_all_features(data)\n",
    "        # Add source prefix to avoid column name conflicts\n",
    "        source_features = source_features.add_prefix(f'{source}_')\n",
    "        news_features = news_features.join(source_features)\n",
    "\n",
    "# Generate sentiment features from social data\n",
    "social_features = pd.DataFrame(index=market_data.index)\n",
    "\n",
    "for source, data in social_data.items():\n",
    "    if not data.empty:\n",
    "        source_features = sentiment_features.calculate_all_features(data)\n",
    "        # Add source prefix\n",
    "        source_features = source_features.add_prefix(f'{source}_')\n",
    "        social_features = social_features.join(source_features)\n",
    "\n",
    "# Combine all sentiment features\n",
    "all_sentiment_features = pd.concat([news_features, social_features], axis=1)\n",
    "\n",
    "# Plot sentiment trends\n",
    "plt.figure(figsize=(15, 6))\n",
    "for column in all_sentiment_features.filter(like='sentiment_score').columns:\n",
    "    all_sentiment_features[column].rolling(window=5).mean().plot(label=column)\n",
    "plt.title('Sentiment Scores Over Time')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot sentiment correlation with returns\n",
    "sentiment_correlation = pd.DataFrame(index=all_sentiment_features.columns)\n",
    "sentiment_correlation['correlation_with_returns'] = [\n",
    "    all_sentiment_features[col].corr(technical_features['Returns'])\n",
    "    for col in all_sentiment_features.columns\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_correlation['correlation_with_returns'].sort_values().plot(kind='bar')\n",
    "plt.title('Sentiment Feature Correlations with Returns')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log sentiment feature generation completion\n",
    "logger.info(f\"Generated {len(all_sentiment_features.columns)} sentiment features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Macroeconomic Feature Generation\n",
    "\n",
    "Generate features from macroeconomic indicators:\n",
    "- Economic indicators and their derivatives\n",
    "- Interest rate features\n",
    "- Growth indicators\n",
    "- Volatility measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize macro feature generator\n",
    "macro_features = MacroFeatures(config)\n",
    "\n",
    "# Generate macro features\n",
    "macro_features_df = macro_features.calculate_all_features(macro_data, market_data)\n",
    "\n",
    "# Display feature information\n",
    "print(\"\\nMacroeconomic Features Overview:\")\n",
    "print(macro_features_df.info())\n",
    "\n",
    "# Plot key macro indicators trends\n",
    "key_macro_features = [\n",
    "    'GDP_YoY', 'CPI_YoY',\n",
    "    'UNRATE_Change', 'FEDFUNDS_Change',\n",
    "    'Economic_Activity_Index', 'Financial_Conditions_Index'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_macro_features):\n",
    "    if feature in macro_features_df.columns:\n",
    "        macro_features_df[feature].plot(ax=axes[idx])\n",
    "        axes[idx].set_title(f'{feature} Over Time')\n",
    "        axes[idx].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Calculate correlations with market returns\n",
    "macro_correlations = pd.DataFrame(index=macro_features_df.columns)\n",
    "macro_correlations['correlation_with_returns'] = [\n",
    "    macro_features_df[col].corr(technical_features['Returns'])\n",
    "    for col in macro_features_df.columns\n",
    "]\n",
    "\n",
    "# Plot top correlations\n",
    "plt.figure(figsize=(12, 6))\n",
    "macro_correlations['correlation_with_returns'].sort_values(ascending=False).head(10).plot(kind='bar')\n",
    "plt.title('Top 10 Macro Features - Correlation with Returns')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log macro feature generation completion\n",
    "logger.info(f\"Generated {len(macro_features_df.columns)} macroeconomic features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Combination and Selection\n",
    "\n",
    "Combine all features and perform feature selection:\n",
    "- Feature aggregation\n",
    "- Feature importance analysis\n",
    "- Correlation analysis\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize feature generator\n",
    "feature_gen = FeatureGenerator(config)\n",
    "\n",
    "# Combine all features\n",
    "all_features = pd.concat([\n",
    "    technical_features,\n",
    "    all_sentiment_features,\n",
    "    macro_features_df\n",
    "], axis=1)\n",
    "\n",
    "# Remove any duplicate columns\n",
    "all_features = all_features.loc[:,~all_features.columns.duplicated()]\n",
    "\n",
    "# Calculate feature importance using Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare target variable (next day returns)\n",
    "target = technical_features['Returns'].shift(-1).dropna()\n",
    "features_for_importance = all_features.loc[target.index]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = pd.DataFrame(\n",
    "    scaler.fit_transform(features_for_importance),\n",
    "    columns=features_for_importance.columns,\n",
    "    index=features_for_importance.index\n",
    ")\n",
    "\n",
    "# Train Random Forest for feature importance\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(scaled_features, target)\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features_for_importance.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate feature correlations\n",
    "correlation_matrix = scaled_features.corr()\n",
    "\n",
    "# Plot correlation heatmap for top features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = feature_importance['feature'].head(15).tolist()\n",
    "sns.heatmap(\n",
    "    correlation_matrix.loc[top_features, top_features],\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    fmt='.2f'\n",
    ")\n",
    "plt.title('Correlation Matrix of Top Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log feature selection completion\n",
    "logger.info(f\"Selected {len(top_features)} top features for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Features and Next Steps\n",
    "\n",
    "Save the engineered features and prepare for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create features directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('data/features', exist_ok=True)\n",
    "\n",
    "# Save all features\n",
    "all_features.to_parquet('data/features/all_features.parquet')\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('data/features/feature_importance.csv')\n",
    "\n",
    "# Save selected features\n",
    "selected_features = all_features[top_features]\n",
    "selected_features.to_parquet('data/features/selected_features.parquet')\n",
    "\n",
    "# Save feature metadata\n",
    "feature_metadata = {\n",
    "    'total_features': len(all_features.columns),\n",
    "    'selected_features': len(top_features),\n",
    "    'technical_features': len(technical_features.columns),\n",
    "    'sentiment_features': len(all_sentiment_features.columns),\n",
    "    'macro_features': len(macro_features_df.columns),\n",
    "    'feature_generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('data/features/feature_metadata.json', 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=4)\n",
    "\n",
    "# Log completion\n",
    "logger.info('Feature engineering completed and saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Review the generated features and their importance scores\n",
    "2. Consider:\n",
    "   - Adding more derived features\n",
    "   - Fine-tuning feature parameters\n",
    "   - Implementing feature selection thresholds\n",
    "3. Proceed to `03_model_development.ipynb` for model training\n",
    "\n",
    "Key Files Generated:\n",
    "- `data/features/all_features.parquet`: Complete feature set\n",
    "- `data/features/selected_features.parquet`: Top selected features\n",
    "- `data/features/feature_importance.csv`: Feature importance scores\n",
    "- `data/features/feature_metadata.json`: Feature generation metadata"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
