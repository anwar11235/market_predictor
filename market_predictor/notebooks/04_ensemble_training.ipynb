{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Predictor: Ensemble Training\n",
    "\n",
    "This notebook focuses on developing and optimizing ensemble models by combining our base models:\n",
    "1. Load Base Models\n",
    "2. Ensemble Architecture Design\n",
    "3. Ensemble Training and Optimization\n",
    "4. Performance Analysis\n",
    "5. Final Model Selection\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models import (\n",
    "    ModelFactory,\n",
    "    EnsembleModel,\n",
    "    create_ensemble\n",
    ")\n",
    "from src.utils import (\n",
    "    setup_project_logger,\n",
    "    ModelMetrics,\n",
    "    TradingMetrics\n",
    ")\n",
    "from config import Config, load_validated_config\n",
    "\n",
    "# Import required libraries\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_project_logger('ensemble_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Models and Data\n",
    "\n",
    "Load the previously trained base models and prepare data for ensemble training:\n",
    "- Load optimized base models\n",
    "- Load processed data\n",
    "- Prepare predictions from base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and data\n",
    "config = load_validated_config('config/parameters.yaml')\n",
    "\n",
    "# Load scaled feature data\n",
    "train_scaled = pd.read_parquet('data/processed/train_scaled.parquet')\n",
    "val_scaled = pd.read_parquet('data/processed/val_scaled.parquet')\n",
    "test_scaled = pd.read_parquet('data/processed/test_scaled.parquet')\n",
    "\n",
    "# Load target data\n",
    "train_target = pd.read_parquet('data/processed/train_target.parquet')\n",
    "val_target = pd.read_parquet('data/processed/val_target.parquet')\n",
    "test_target = pd.read_parquet('data/processed/test_target.parquet')\n",
    "\n",
    "# Load base models and their predictions\n",
    "base_models = {}\n",
    "base_predictions = {}\n",
    "model_names = ['random_forest', 'xgboost', 'lightgbm']\n",
    "\n",
    "for model_name in model_names:\n",
    "    # Load model\n",
    "    model_path = f'models/{model_name}_optimized.joblib'\n",
    "    base_models[model_name] = joblib.load(model_path)\n",
    "    \n",
    "    # Get predictions\n",
    "    base_predictions[model_name] = {\n",
    "        'train': base_models[model_name].predict_proba(train_scaled),\n",
    "        'val': base_models[model_name].predict_proba(val_scaled),\n",
    "        'test': base_models[model_name].predict_proba(test_scaled)\n",
    "    }\n",
    "\n",
    "# Print base model performances\n",
    "print(\"\\nBase Model Performances (Validation Set):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in model_names:\n",
    "    val_preds = base_predictions[model_name]['val']\n",
    "    val_class_preds = np.argmax(val_preds, axis=1)\n",
    "    \n",
    "    metrics = ModelMetrics.classification_metrics(\n",
    "        val_target,\n",
    "        val_class_preds,\n",
    "        val_preds\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "\n",
    "# Plot prediction correlation heatmap\n",
    "val_predictions_df = pd.DataFrame({\n",
    "    name: preds['val'][:, 1] for name, preds in base_predictions.items()\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    val_predictions_df.corr(),\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1\n",
    ")\n",
    "plt.title('Base Model Prediction Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info('Base models and data loaded successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensemble Architecture Design\n",
    "\n",
    "Design and implement different ensemble architectures:\n",
    "- Voting Ensemble\n",
    "- Weighted Ensemble\n",
    "- Stacking Ensemble\n",
    "Compare their initial performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ensemble configurations\n",
    "ensemble_configs = {\n",
    "    'voting': {\n",
    "        'type': 'voting',\n",
    "        'params': {\n",
    "            'voting': 'soft',\n",
    "            'weights': None\n",
    "        }\n",
    "    },\n",
    "    'weighted': {\n",
    "        'type': 'weighted',\n",
    "        'params': {\n",
    "            'voting': 'soft',\n",
    "            'weights': None,  # Will be optimized\n",
    "            'dynamic_weights': True\n",
    "        }\n",
    "    },\n",
    "    'stacking': {\n",
    "        'type': 'stacking',\n",
    "        'params': {\n",
    "            'meta_model': 'lightgbm',\n",
    "            'use_features': True,\n",
    "            'cv': 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize ensembles\n",
    "ensembles = {}\n",
    "ensemble_predictions = {}\n",
    "\n",
    "for name, config in ensemble_configs.items():\n",
    "    print(f\"\\nInitializing {name} ensemble...\")\n",
    "    \n",
    "    # Create ensemble\n",
    "    ensemble = create_ensemble(\n",
    "        config=config,\n",
    "        base_models=base_models,\n",
    "        model_names=model_names\n",
    "    )\n",
    "    \n",
    "    # Store ensemble\n",
    "    ensembles[name] = ensemble\n",
    "\n",
    "# Train and evaluate each ensemble\n",
    "for name, ensemble in ensembles.items():\n",
    "    print(f\"\\nTraining {name} ensemble...\")\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.train(\n",
    "        X_train=train_scaled,\n",
    "        y_train=train_target,\n",
    "        X_val=val_scaled,\n",
    "        y_val=val_target,\n",
    "        base_predictions={\n",
    "            'train': {model: preds['train'] for model, preds in base_predictions.items()},\n",
    "            'val': {model: preds['val'] for model, preds in base_predictions.items()}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    train_preds = ensemble.predict_proba(train_scaled)\n",
    "    val_preds = ensemble.predict_proba(val_scaled)\n",
    "    \n",
    "    ensemble_predictions[name] = {\n",
    "        'train': train_preds,\n",
    "        'val': val_preds\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = ModelMetrics.classification_metrics(\n",
    "        train_target,\n",
    "        np.argmax(train_preds, axis=1),\n",
    "        train_preds\n",
    "    )\n",
    "    \n",
    "    val_metrics = ModelMetrics.classification_metrics(\n",
    "        val_target,\n",
    "        np.argmax(val_preds, axis=1),\n",
    "        val_preds\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{name.upper()} ENSEMBLE RESULTS:\")\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {train_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "# Plot ensemble performances comparison\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "comparison_data = []\n",
    "\n",
    "for ensemble_name in ensembles.keys():\n",
    "    val_preds = ensemble_predictions[ensemble_name]['val']\n",
    "    val_class_preds = np.argmax(val_preds, axis=1)\n",
    "    \n",
    "    metrics_dict = ModelMetrics.classification_metrics(\n",
    "        val_target,\n",
    "        val_class_preds,\n",
    "        val_preds\n",
    "    )\n",
    "    \n",
    "    for metric in metrics:\n",
    "        comparison_data.append({\n",
    "            'Ensemble': ensemble_name,\n",
    "            'Metric': metric,\n",
    "            'Value': metrics_dict[metric]\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=comparison_df,\n",
    "    x='Metric',\n",
    "    y='Value',\n",
    "    hue='Ensemble'\n",
    ")\n",
    "plt.title('Ensemble Models Performance Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info('Ensemble architectures evaluated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble Optimization\n",
    "\n",
    "Optimize the best performing ensemble:\n",
    "- Weight optimization\n",
    "- Meta-model tuning\n",
    "- Cross-validation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best performing ensemble\n",
    "val_scores = {}\n",
    "for name, ensemble in ensembles.items():\n",
    "    val_preds = ensemble_predictions[name]['val']\n",
    "    val_class_preds = np.argmax(val_preds, axis=1)\n",
    "    \n",
    "    metrics = ModelMetrics.classification_metrics(\n",
    "        val_target,\n",
    "        val_class_preds,\n",
    "        val_preds\n",
    "    )\n",
    "    val_scores[name] = metrics['f1']\n",
    "\n",
    "best_ensemble_name = max(val_scores, key=val_scores.get)\n",
    "print(f\"Best performing ensemble: {best_ensemble_name}\")\n",
    "\n",
    "# Optimize weights for the best ensemble\n",
    "def optimize_ensemble_weights(ensemble, X, y, base_preds, n_trials=50):\n",
    "    best_score = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        # Generate random weights\n",
    "        weights = np.random.dirichlet(np.ones(len(base_models)))\n",
    "        \n",
    "        # Update ensemble weights\n",
    "        ensemble.set_weights(weights)\n",
    "        \n",
    "        # Get predictions\n",
    "        preds = ensemble.predict_proba(X)\n",
    "        class_preds = np.argmax(preds, axis=1)\n",
    "        \n",
    "        # Calculate score\n",
    "        metrics = ModelMetrics.classification_metrics(y, class_preds, preds)\n",
    "        score = metrics['f1']\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_weights = weights\n",
    "    \n",
    "    return best_weights, best_score\n",
    "\n",
    "# Optimize best ensemble\n",
    "best_ensemble = ensembles[best_ensemble_name]\n",
    "print(\"\\nOptimizing ensemble weights...\")\n",
    "\n",
    "optimal_weights, optimal_score = optimize_ensemble_weights(\n",
    "    best_ensemble,\n",
    "    val_scaled,\n",
    "    val_target,\n",
    "    {model: preds['val'] for model, preds in base_predictions.items()}\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal weights found (F1: {optimal_score:.4f}):\")\n",
    "for model_name, weight in zip(model_names, optimal_weights):\n",
    "    print(f\"{model_name}: {weight:.4f}\")\n",
    "\n",
    "# Update ensemble with optimal weights\n",
    "best_ensemble.set_weights(optimal_weights)\n",
    "\n",
    "# Perform cross-validation analysis\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def cv_analysis(ensemble, X, y, n_splits=5):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        # Split data\n",
    "        cv_X_train = X.iloc[train_idx]\n",
    "        cv_y_train = y.iloc[train_idx]\n",
    "        cv_X_val = X.iloc[val_idx]\n",
    "        cv_y_val = y.iloc[val_idx]\n",
    "        \n",
    "        # Train ensemble\n",
    "        ensemble.train(cv_X_train, cv_y_train, cv_X_val, cv_y_val)\n",
    "        \n",
    "        # Get predictions\n",
    "        val_preds = ensemble.predict_proba(cv_X_val)\n",
    "        val_class_preds = np.argmax(val_preds, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = ModelMetrics.classification_metrics(\n",
    "            cv_y_val,\n",
    "            val_class_preds,\n",
    "            val_preds\n",
    "        )\n",
    "        cv_scores.append(metrics)\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cv_analysis(best_ensemble, train_scaled, train_target)\n",
    "\n",
    "# Plot CV results\n",
    "cv_metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results = []\n",
    "\n",
    "for fold, metrics in enumerate(cv_scores):\n",
    "    for metric in cv_metrics:\n",
    "        cv_results.append({\n",
    "            'Fold': fold + 1,\n",
    "            'Metric': metric,\n",
    "            'Score': metrics[metric]\n",
    "        })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=cv_df, x='Metric', y='Score')\n",
    "plt.title('Cross-validation Results')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info('Ensemble optimization completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of the optimized ensemble model:\n",
    "- Performance metrics\n",
    "- Trading metrics\n",
    "- Risk analysis\n",
    "- Comparison with base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions with optimized ensemble\n",
    "final_predictions = {\n",
    "    'train': best_ensemble.predict_proba(train_scaled),\n",
    "    'val': best_ensemble.predict_proba(val_scaled),\n",
    "    'test': best_ensemble.predict_proba(test_scaled)\n",
    "}\n",
    "\n",
    "class_predictions = {\n",
    "    dataset: np.argmax(preds, axis=1)\n",
    "    for dataset, preds in final_predictions.items()\n",
    "}\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "datasets = {\n",
    "    'train': (train_target, train_scaled),\n",
    "    'val': (val_target, val_scaled),\n",
    "    'test': (test_target, test_scaled)\n",
    "}\n",
    "\n",
    "final_metrics = {}\n",
    "for dataset_name, (y_true, X) in datasets.items():\n",
    "    y_pred = class_predictions[dataset_name]\n",
    "    y_prob = final_predictions[dataset_name]\n",
    "    \n",
    "    # Classification metrics\n",
    "    class_metrics = ModelMetrics.classification_metrics(y_true, y_pred, y_prob)\n",
    "    \n",
    "    # Trading metrics\n",
    "    returns = pd.Series(index=X.index)  # Replace with actual returns\n",
    "    trade_metrics = TradingMetrics.calculate_metrics(returns, y_pred)\n",
    "    \n",
    "    final_metrics[dataset_name] = {\n",
    "        'classification': class_metrics,\n",
    "        'trading': trade_metrics\n",
    "    }\n",
    "\n",
    "# Plot comprehensive results\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Classification metrics plot\n",
    "plt.subplot(2, 2, 1)\n",
    "class_data = []\n",
    "for dataset, metrics in final_metrics.items():\n",
    "    for metric, value in metrics['classification'].items():\n",
    "        class_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Metric': metric,\n",
    "            'Value': value\n",
    "        })\n",
    "class_df = pd.DataFrame(class_data)\n",
    "sns.barplot(data=class_df, x='Metric', y='Value', hue='Dataset')\n",
    "plt.title('Classification Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Trading metrics plot\n",
    "plt.subplot(2, 2, 2)\n",
    "trade_data = []\n",
    "for dataset, metrics in final_metrics.items():\n",
    "    for metric, value in metrics['trading'].items():\n",
    "        trade_data.append({\n",
    "            'Dataset': dataset,\n",
    "            'Metric': metric,\n",
    "            'Value': value\n",
    "        })\n",
    "trade_df = pd.DataFrame(trade_data)\n",
    "sns.barplot(data=trade_df, x='Metric', y='Value', hue='Dataset')\n",
    "plt.title('Trading Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Confusion matrix plot\n",
    "plt.subplot(2, 2, 3)\n",
    "cm = confusion_matrix(test_target, class_predictions['test'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Test Set Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# ROC curve plot\n",
    "plt.subplot(2, 2, 4)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(test_target, final_predictions['test'][:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title('ROC Curve (Test Set)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save final model and results\n",
    "joblib.dump(best_ensemble, 'models/final_ensemble.joblib')\n",
    "\n",
    "final_results = {\n",
    "    'model_type': best_ensemble_name,\n",
    "    'optimal_weights': optimal_weights.tolist(),\n",
    "    'metrics': final_metrics,\n",
    "    'cv_results': cv_results,\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "with open('models/final_ensemble_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=4)\n",
    "\n",
    "logger.info('Final model evaluation completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Export and Next Steps\n",
    "\n",
    "Prepare the final ensemble model for deployment:\n",
    "- Export model artifacts\n",
    "- Document model usage\n",
    "- Define monitoring metrics\n",
    "- Outline deployment steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model deployment package\n",
    "deployment_package = {\n",
    "    'model': {\n",
    "        'ensemble_path': 'models/final_ensemble.joblib',\n",
    "        'base_models': {\n",
    "            name: f'models/{name}_base.joblib'\n",
    "            for name in model_names\n",
    "        },\n",
    "        'scaler_path': 'models/feature_scaler.joblib'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'features': train_scaled.columns.tolist(),\n",
    "        'model_version': '1.0.0',\n",
    "        'python_version': sys.version,\n",
    "        'required_packages': {\n",
    "            'numpy': np.__version__,\n",
    "            'pandas': pd.__version__,\n",
    "            'scikit-learn': sklearn.__version__\n",
    "        }\n",
    "    },\n",
    "    'performance': final_metrics,\n",
    "    'configuration': {\n",
    "        'ensemble_type': best_ensemble_name,\n",
    "        'model_weights': optimal_weights.tolist(),\n",
    "        'prediction_threshold': 0.5,\n",
    "        'update_frequency': '24h'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save deployment package\n",
    "with open('models/deployment_package.json', 'w') as f:\n",
    "    json.dump(deployment_package, f, indent=4)\n",
    "\n",
    "# Create model usage example\n",
    "usage_example = \"\"\"\n",
    "# Model Usage Example\n",
    "from src.models import load_ensemble_model\n",
    "import pandas as pd\n",
    "\n",
    "# Load model and dependencies\n",
    "model = load_ensemble_model('models/final_ensemble.joblib')\n",
    "scaler = joblib.load('models/feature_scaler.joblib')\n",
    "\n",
    "# Prepare features\n",
    "def prepare_features(data):\n",
    "    # Add feature preparation code\n",
    "    return processed_features\n",
    "\n",
    "# Make predictions\n",
    "def predict_market_direction(data):\n",
    "    features = prepare_features(data)\n",
    "    scaled_features = scaler.transform(features)\n",
    "    predictions = model.predict_proba(scaled_features)\n",
    "    return predictions\n",
    "\"\"\"\n",
    "\n",
    "with open('models/usage_example.py', 'w') as f:\n",
    "    f.write(usage_example)\n",
    "\n",
    "# Print deployment checklist\n",
    "print(\"\\nDeployment Checklist:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. Model Artifacts:\")\n",
    "print(\"   - Final ensemble model saved\")\n",
    "print(\"   - Base models saved\")\n",
    "print(\"   - Feature scaler saved\")\n",
    "print(\"   - Model metadata documented\")\n",
    "\n",
    "print(\"\\n2. Performance Validation:\")\n",
    "print(\"   - Cross-validation completed\")\n",
    "print(\"   - Test set performance verified\")\n",
    "print(\"   - Trading metrics calculated\")\n",
    "\n",
    "print(\"\\n3. Monitoring Setup:\")\n",
    "print(\"   - Performance metrics defined\")\n",
    "print(\"   - Alert thresholds established\")\n",
    "print(\"   - Logging configured\")\n",
    "\n",
    "print(\"\\n4. Next Steps:\")\n",
    "print(\"   - Set up model API\")\n",
    "print(\"   - Implement real-time data pipeline\")\n",
    "print(\"   - Configure monitoring dashboard\")\n",
    "print(\"   - Establish retraining schedule\")\n",
    "\n",
    "logger.info('Model export and deployment preparation completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Model Deployment:\n",
    "   - Set up model serving infrastructure\n",
    "   - Implement API endpoints\n",
    "   - Configure monitoring and alerting\n",
    "   - Establish backup and failover procedures\n",
    "\n",
    "2. Production Pipeline:\n",
    "   - Real-time data collection\n",
    "   - Automated feature generation\n",
    "   - Prediction scheduling\n",
    "   - Results logging and analysis\n",
    "\n",
    "3. Monitoring and Maintenance:\n",
    "   - Performance monitoring dashboard\n",
    "   - Data drift detection\n",
    "   - Model retraining triggers\n",
    "   - Alert configuration\n",
    "\n",
    "4. Future Improvements:\n",
    "   - Additional data sources\n",
    "   - Feature engineering refinements\n",
    "   - Model architecture experiments\n",
    "   - Risk management enhancements\n",
    "\n",
    "Proceed to `05_backtesting.ipynb` for comprehensive strategy testing."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
