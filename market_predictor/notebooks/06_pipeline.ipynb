{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anwar11235/market_predictor/blob/main/market_predictor/notebooks/06_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Market Predictor Pipeline Orchestration\n",
        "\n",
        "This notebook orchestrates the complete market prediction pipeline:\n",
        "1. Data Collection\n",
        "2. Feature Engineering\n",
        "3. Model Training\n",
        "4. Strategy Backtesting\n",
        "5. Performance Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu77xhskvnZ5",
        "outputId": "09ce991c-d780-49f5-f99e-1d463ff72550"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append('..')\n",
        "\n",
        "# Import project components\n",
        "from src.data import DataLoader\n",
        "from src.features import FeatureGenerator\n",
        "from src.models import create_ensemble, ModelFactory\n",
        "from src.integrations import create_data_clients\n",
        "from src.utils import setup_project_logger\n",
        "from config import Config, load_validated_config\n",
        "\n",
        "# Set up logging\n",
        "logger = setup_project_logger('pipeline_orchestration')\n",
        "\n",
        "# Plotting settings\n",
        "plt.style.use('seaborn')\n",
        "%matplotlib inline\n",
        "sns.set_theme(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Collection and Integration\n",
        "\n",
        "Initialize data sources and collect data from all providers:\n",
        "- Market data\n",
        "- News and sentiment data\n",
        "- Macroeconomic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config = load_validated_config('config/parameters.yaml')\n",
        "\n",
        "# Initialize data clients\n",
        "logger.info(\"Initializing data clients...\")\n",
        "clients = create_data_clients(config)\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = DataLoader(config)\n",
        "\n",
        "# Collect market data\n",
        "logger.info(\"Collecting market data...\")\n",
        "market_data = data_loader.get_market_data()\n",
        "print(\"\\nMarket Data Overview:\")\n",
        "print(market_data.info())\n",
        "\n",
        "# Collect news and sentiment data\n",
        "logger.info(\"Collecting news and sentiment data...\")\n",
        "news_data = {}\n",
        "sentiment_data = {}\n",
        "\n",
        "if 'newsapi' in clients:\n",
        "    news_data['newsapi'] = clients['newsapi'].get_market_news()\n",
        "if 'finnhub' in clients:\n",
        "    news_data['finnhub'] = clients['finnhub'].get_market_news()\n",
        "\n",
        "# Collect macroeconomic data\n",
        "logger.info(\"Collecting macroeconomic data...\")\n",
        "macro_data = data_loader.get_macro_data()\n",
        "print(\"\\nMacroeconomic Data Overview:\")\n",
        "print(macro_data.info())\n",
        "\n",
        "# Plot data overview\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "# Market data plot\n",
        "market_data['Close'].plot(ax=axes[0])\n",
        "axes[0].set_title('Market Price')\n",
        "axes[0].set_ylabel('Price')\n",
        "\n",
        "# Volume plot\n",
        "market_data['Volume'].plot(ax=axes[1])\n",
        "axes[1].set_title('Trading Volume')\n",
        "axes[1].set_ylabel('Volume')\n",
        "\n",
        "# Macro indicators plot\n",
        "if not macro_data.empty:\n",
        "    macro_data.iloc[:, 0].plot(ax=axes[2])  # Plot first macro indicator\n",
        "    axes[2].set_title('Macroeconomic Indicator')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save collected data\n",
        "logger.info(\"Saving collected data...\")\n",
        "market_data.to_parquet('data/processed/market_data.parquet')\n",
        "macro_data.to_parquet('data/processed/macro_data.parquet')\n",
        "\n",
        "for source, data in news_data.items():\n",
        "    if not data.empty:\n",
        "        data.to_parquet(f'data/processed/news_{source}.parquet')\n",
        "\n",
        "logger.info(\"Data collection completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajB65Ywy-rP"
      },
      "source": [
        "## 1. Data Collection and Integration\n",
        "\n",
        "Initialize data sources and collect data from all providers:\n",
        "- Market data\n",
        "- News and sentiment data\n",
        "- Macroeconomic data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "config = load_validated_config('config/parameters.yaml')\n",
        "\n",
        "# Initialize data clients\n",
        "logger.info(\"Initializing data clients...\")\n",
        "clients = create_data_clients(config)\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = DataLoader(config)\n",
        "\n",
        "# Collect market data\n",
        "logger.info(\"Collecting market data...\")\n",
        "market_data = data_loader.get_market_data()\n",
        "print(\"\\nMarket Data Overview:\")\n",
        "print(market_data.info())\n",
        "\n",
        "# Collect news and sentiment data\n",
        "logger.info(\"Collecting news and sentiment data...\")\n",
        "news_data = {}\n",
        "sentiment_data = {}\n",
        "\n",
        "if 'newsapi' in clients:\n",
        "    news_data['newsapi'] = clients['newsapi'].get_market_news()\n",
        "if 'finnhub' in clients:\n",
        "    news_data['finnhub'] = clients['finnhub'].get_market_news()\n",
        "\n",
        "# Collect macroeconomic data\n",
        "logger.info(\"Collecting macroeconomic data...\")\n",
        "macro_data = data_loader.get_macro_data()\n",
        "print(\"\\nMacroeconomic Data Overview:\")\n",
        "print(macro_data.info())\n",
        "\n",
        "# Plot data overview\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
        "\n",
        "# Market data plot\n",
        "market_data['Close'].plot(ax=axes[0])\n",
        "axes[0].set_title('Market Price')\n",
        "axes[0].set_ylabel('Price')\n",
        "\n",
        "# Volume plot\n",
        "market_data['Volume'].plot(ax=axes[1])\n",
        "axes[1].set_title('Trading Volume')\n",
        "axes[1].set_ylabel('Volume')\n",
        "\n",
        "# Macro indicators plot\n",
        "if not macro_data.empty:\n",
        "    macro_data.iloc[:, 0].plot(ax=axes[2])  # Plot first macro indicator\n",
        "    axes[2].set_title('Macroeconomic Indicator')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save collected data\n",
        "logger.info(\"Saving collected data...\")\n",
        "market_data.to_parquet('data/processed/market_data.parquet')\n",
        "macro_data.to_parquet('data/processed/macro_data.parquet')\n",
        "\n",
        "for source, data in news_data.items():\n",
        "    if not data.empty:\n",
        "        data.to_parquet(f'data/processed/news_{source}.parquet')\n",
        "\n",
        "logger.info(\"Data collection completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n",
        "\n",
        "Generate and process features from all data sources:\n",
        "- Technical features\n",
        "- Sentiment features\n",
        "- Macroeconomic features\n",
        "- Feature selection and combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize feature generator\n",
        "logger.info(\"Initializing feature generation...\")\n",
        "feature_gen = FeatureGenerator(config)\n",
        "\n",
        "# Generate technical features\n",
        "logger.info(\"Generating technical features...\")\n",
        "tech_features = feature_gen.technical_features.calculate_all_features(market_data)\n",
        "print(\"\\nTechnical Features Overview:\")\n",
        "print(tech_features.info())\n",
        "\n",
        "# Generate sentiment features\n",
        "logger.info(\"Generating sentiment features...\")\n",
        "sent_features = pd.DataFrame(index=market_data.index)\n",
        "for source, data in news_data.items():\n",
        "    if not data.empty:\n",
        "        source_features = feature_gen.sentiment_features.calculate_all_features(data)\n",
        "        sent_features = pd.concat([sent_features, source_features], axis=1)\n",
        "\n",
        "print(\"\\nSentiment Features Overview:\")\n",
        "print(sent_features.info())\n",
        "\n",
        "# Generate macro features\n",
        "logger.info(\"Generating macro features...\")\n",
        "macro_features = feature_gen.macro_features.calculate_all_features(\n",
        "    macro_data,\n",
        "    market_data\n",
        ")\n",
        "print(\"\\nMacro Features Overview:\")\n",
        "print(macro_features.info())\n",
        "\n",
        "# Combine all features\n",
        "logger.info(\"Combining features...\")\n",
        "all_features = feature_gen.generate_all_features(\n",
        "    market_data=market_data,\n",
        "    macro_data=macro_data,\n",
        "    sentiment_data=sent_features\n",
        ")\n",
        "\n",
        "# Create target variable (next day return direction)\n",
        "returns = market_data['Close'].pct_change()\n",
        "target = np.where(returns.shift(-1) > 0, 1, 0)\n",
        "target = pd.Series(target[:-1], index=returns.index[:-1])\n",
        "\n",
        "# Feature selection\n",
        "logger.info(\"Performing feature selection...\")\n",
        "selected_features = feature_gen.select_features(\n",
        "    all_features.loc[target.index],\n",
        "    target,\n",
        "    n_features=20\n",
        ")\n",
        "\n",
        "# Plot feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': selected_features.columns,\n",
        "    'importance': feature_gen.feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
        "plt.title('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(\n",
        "    selected_features.corr(),\n",
        "    annot=True,\n",
        "    cmap='coolwarm',\n",
        "    center=0,\n",
        "    fmt='.2f'\n",
        ")\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save processed features\n",
        "logger.info(\"Saving processed features...\")\n",
        "selected_features.to_parquet('data/features/selected_features.parquet')\n",
        "feature_importance.to_csv('data/features/feature_importance.csv')\n",
        "\n",
        "# Save feature metadata\n",
        "feature_metadata = {\n",
        "    'n_technical_features': len(tech_features.columns),\n",
        "    'n_sentiment_features': len(sent_features.columns),\n",
        "    'n_macro_features': len(macro_features.columns),\n",
        "    'n_selected_features': len(selected_features.columns),\n",
        "    'feature_names': selected_features.columns.tolist(),\n",
        "    'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}\n",
        "\n",
        "with open('data/features/feature_metadata.json', 'w') as f:\n",
        "    json.dump(feature_metadata, f, indent=4)\n",
        "\n",
        "logger.info(\"Feature engineering completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training and Ensemble Creation\n",
        "\n",
        "Train base models and create ensemble:\n",
        "- Train individual models\n",
        "- Optimize hyperparameters\n",
        "- Create and train ensemble\n",
        "- Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train, validation, and test sets\n",
        "logger.info(\"Preparing training data...\")\n",
        "\n",
        "train_end = pd.Timestamp(config.data.validation_start)\n",
        "val_end = pd.Timestamp(config.data.test_start)\n",
        "\n",
        "# Features\n",
        "train_features = selected_features[:train_end]\n",
        "val_features = selected_features[train_end:val_end]\n",
        "test_features = selected_features[val_end:]\n",
        "\n",
        "# Target\n",
        "train_target = target[:train_end]\n",
        "val_target = target[train_end:val_end]\n",
        "test_target = target[val_end:]\n",
        "\n",
        "# Initialize model factory\n",
        "model_factory = ModelFactory(config)\n",
        "\n",
        "# Train base models\n",
        "logger.info(\"Training base models...\")\n",
        "base_models = {}\n",
        "base_predictions = {}\n",
        "model_metrics = {}\n",
        "\n",
        "for model_type in ['random_forest', 'xgboost', 'lightgbm']:\n",
        "    logger.info(f\"Training {model_type}...\")\n",
        "    \n",
        "    # Create and train model\n",
        "    model = model_factory.create_model(model_type)\n",
        "    history = model.train(\n",
        "        train_features,\n",
        "        train_target,\n",
        "        val_features,\n",
        "        val_target\n",
        "    )\n",
        "    \n",
        "    # Store model and predictions\n",
        "    base_models[model_type] = model\n",
        "    base_predictions[model_type] = {\n",
        "        'train': model.predict_proba(train_features),\n",
        "        'val': model.predict_proba(val_features),\n",
        "        'test': model.predict_proba(test_features)\n",
        "    }\n",
        "    model_metrics[model_type] = history\n",
        "\n",
        "# Create and train ensemble\n",
        "logger.info(\"Creating ensemble model...\")\n",
        "ensemble = create_ensemble(\n",
        "    config=config,\n",
        "    base_models=list(base_models.values()),\n",
        "    model_names=list(base_models.keys())\n",
        ")\n",
        "\n",
        "# Train ensemble\n",
        "ensemble_history = ensemble.train(\n",
        "    train_features,\n",
        "    train_target,\n",
        "    val_features,\n",
        "    val_target,\n",
        "    base_predictions={\n",
        "        'train': {name: pred['train'] for name, pred in base_predictions.items()},\n",
        "        'val': {name: pred['val'] for name, pred in base_predictions.items()}\n",
        "    }\n",
        ")\n",
        "\n",
        "# Get ensemble predictions\n",
        "ensemble_predictions = {\n",
        "    'train': ensemble.predict_proba(train_features),\n",
        "    'val': ensemble.predict_proba(val_features),\n",
        "    'test': ensemble.predict_proba(test_features)\n",
        "}\n",
        "\n",
        "# Evaluate models\n",
        "def evaluate_predictions(y_true, y_prob):\n",
        "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "    y_pred = (y_prob[:, 1] > 0.5).astype(int)\n",
        "    return {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred),\n",
        "        'auc_roc': roc_auc_score(y_true, y_prob[:, 1])\n",
        "    }\n",
        "\n",
        "evaluation_results = {}\n",
        "for name in base_models:\n",
        "    evaluation_results[name] = {\n",
        "        'train': evaluate_predictions(train_target, base_predictions[name]['train']),\n",
        "        'val': evaluate_predictions(val_target, base_predictions[name]['val']),\n",
        "        'test': evaluate_predictions(test_target, base_predictions[name]['test'])\n",
        "    }\n",
        "\n",
        "evaluation_results['ensemble'] = {\n",
        "    'train': evaluate_predictions(train_target, ensemble_predictions['train']),\n",
        "    'val': evaluate_predictions(val_target, ensemble_predictions['val']),\n",
        "    'test': evaluate_predictions(test_target, ensemble_predictions['test'])\n",
        "}\n",
        "\n",
        "# Plot evaluation results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    data = []\n",
        "    for model in evaluation_results:\n",
        "        for dataset in ['train', 'val', 'test']:\n",
        "            data.append({\n",
        "                'Model': model,\n",
        "                'Dataset': dataset,\n",
        "                'Score': evaluation_results[model][dataset][metric]\n",
        "            })\n",
        "    df = pd.DataFrame(data)\n",
        "    sns.barplot(x='Model', y='Score', hue='Dataset', data=df, ax=ax)\n",
        "    ax.set_title(f'{metric.capitalize()} Score')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save models and results\n",
        "logger.info(\"Saving models and results...\")\n",
        "joblib.dump(ensemble, 'models/final_ensemble.joblib')\n",
        "for name, model in base_models.items():\n",
        "    joblib.dump(model, f'models/{name}_base.joblib')\n",
        "\n",
        "with open('models/evaluation_results.json', 'w') as f:\n",
        "    json.dump(evaluation_results, f, indent=4)\n",
        "\n",
        "logger.info(\"Model training completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Backtesting and Performance Analysis\n",
        "\n",
        "Backtest the trading strategy and analyze performance:\n",
        "- Strategy implementation\n",
        "- Historical simulation\n",
        "- Performance metrics\n",
        "- Risk analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize backtesting parameters\n",
        "logger.info(\"Setting up backtesting...\")\n",
        "\n",
        "backtest_config = {\n",
        "    'initial_capital': 100000,\n",
        "    'position_size': 0.1,  # 10% of portfolio per trade\n",
        "    'stop_loss': 0.02,     # 2% stop loss\n",
        "    'take_profit': 0.03,   # 3% take profit\n",
        "    'max_positions': 5,    # Maximum number of simultaneous positions\n",
        "    'transaction_costs': 0.001  # 0.1% transaction cost\n",
        "}\n",
        "\n",
        "class StrategyBacktest:\n",
        "    def __init__(self, model, features, market_data, config):\n",
        "        self.model = model\n",
        "        self.features = features\n",
        "        self.market_data = market_data\n",
        "        self.config = config\n",
        "        self.reset()\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset backtest state\"\"\"\n",
        "        self.portfolio_value = []\n",
        "        self.positions = {}\n",
        "        self.capital = self.config['initial_capital']\n",
        "        self.available_capital = self.config['initial_capital']\n",
        "        \n",
        "    def run(self):\n",
        "        \"\"\"Run backtest simulation\"\"\"\n",
        "        for date in self.features.index:\n",
        "            # Get prediction\n",
        "            features = self.features.loc[date].values.reshape(1, -1)\n",
        "            prob = self.model.predict_proba(features)[0, 1]\n",
        "            \n",
        "            # Update existing positions\n",
        "            self._update_positions(date)\n",
        "            \n",
        "            # Check for new position\n",
        "            if prob > 0.6 and len(self.positions) < self.config['max_positions']:\n",
        "                self._open_position(date)\n",
        "            \n",
        "            # Record portfolio value\n",
        "            total_value = self._calculate_portfolio_value(date)\n",
        "            self.portfolio_value.append({\n",
        "                'date': date,\n",
        "                'value': total_value,\n",
        "                'cash': self.available_capital,\n",
        "                'positions': len(self.positions)\n",
        "            })\n",
        "    \n",
        "    def _update_positions(self, date):\n",
        "        \"\"\"Update existing positions\"\"\"\n",
        "        current_price = self.market_data.loc[date, 'Close']\n",
        "        closed_positions = []\n",
        "        \n",
        "        for entry_date, position in self.positions.items():\n",
        "            # Check stop loss\n",
        "            if current_price <= position['stop_price']:\n",
        "                self._close_position(entry_date, current_price, 'stop_loss')\n",
        "                closed_positions.append(entry_date)\n",
        "                \n",
        "            # Check take profit\n",
        "            elif current_price >= position['target_price']:\n",
        "                self._close_position(entry_date, current_price, 'take_profit')\n",
        "                closed_positions.append(entry_date)\n",
        "        \n",
        "        # Remove closed positions\n",
        "        for date in closed_positions:\n",
        "            del self.positions[date]\n",
        "    \n",
        "    def _open_position(self, date):\n",
        "        \"\"\"Open new position\"\"\"\n",
        "        current_price = self.market_data.loc[date, 'Close']\n",
        "        position_value = self.available_capital * self.config['position_size']\n",
        "        shares = position_value // current_price\n",
        "        \n",
        "        if shares > 0:\n",
        "            cost = shares * current_price\n",
        "            self.positions[date] = {\n",
        "                'shares': shares,\n",
        "                'entry_price': current_price,\n",
        "                'stop_price': current_price * (1 - self.config['stop_loss']),\n",
        "                'target_price': current_price * (1 + self.config['take_profit'])\n",
        "            }\n",
        "            self.available_capital -= cost\n",
        "    \n",
        "    def _calculate_portfolio_value(self, date):\n",
        "        \"\"\"Calculate total portfolio value\"\"\"\n",
        "        current_price = self.market_data.loc[date, 'Close']\n",
        "        position_value = sum(\n",
        "            pos['shares'] * current_price \n",
        "            for pos in self.positions.values()\n",
        "        )\n",
        "        return self.available_capital + position_value\n",
        "    \n",
        "    def get_results(self):\n",
        "        \"\"\"Get backtest results\"\"\"\n",
        "        results = pd.DataFrame(self.portfolio_value)\n",
        "        results.set_index('date', inplace=True)\n",
        "        return results\n",
        "\n",
        "# Run backtest\n",
        "logger.info(\"Running backtest simulation...\")\n",
        "backtest = StrategyBacktest(ensemble, test_features, market_data.loc[test_features.index], backtest_config)\n",
        "backtest.run()\n",
        "backtest_results = backtest.get_results()\n",
        "\n",
        "# Calculate performance metrics\n",
        "def calculate_performance_metrics(results):\n",
        "    \"\"\"Calculate trading performance metrics\"\"\"\n",
        "    returns = results['value'].pct_change()\n",
        "    \n",
        "    return {\n",
        "        'Total Return (%)': (results['value'].iloc[-1] / results['value'].iloc[0] - 1) * 100,\n",
        "        'Annual Return (%)': returns.mean() * 252 * 100,\n",
        "        'Volatility (%)': returns.std() * np.sqrt(252) * 100,\n",
        "        'Sharpe Ratio': (returns.mean() * 252) / (returns.std() * np.sqrt(252)),\n",
        "        'Max Drawdown (%)': ((results['value'] / results['value'].cummax() - 1).min()) * 100,\n",
        "        'Win Rate (%)': (returns > 0).mean() * 100\n",
        "    }\n",
        "\n",
        "metrics = calculate_performance_metrics(backtest_results)\n",
        "\n",
        "# Plot results\n",
        "fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
        "\n",
        "# Portfolio value\n",
        "backtest_results['value'].plot(ax=axes[0])\n",
        "axes[0].set_title('Portfolio Value')\n",
        "axes[0].set_ylabel('Value ($)')\n",
        "\n",
        "# Number of positions\n",
        "backtest_results['positions'].plot(ax=axes[1])\n",
        "axes[1].set_title('Number of Active Positions')\n",
        "axes[1].set_ylabel('Positions')\n",
        "\n",
        "# Drawdown\n",
        "drawdown = (backtest_results['value'] / backtest_results['value'].cummax() - 1) * 100\n",
        "drawdown.plot(ax=axes[2])\n",
        "axes[2].set_title('Drawdown')\n",
        "axes[2].set_ylabel('Drawdown (%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nPerformance Metrics:\")\n",
        "print(\"=\" * 50)\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "# Save results\n",
        "logger.info(\"Saving backtest results...\")\n",
        "backtest_results.to_parquet('results/backtest_results.parquet')\n",
        "with open('results/performance_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "logger.info(\"Backtesting completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Monitoring and Deployment Preparation\n",
        "\n",
        "Prepare the model for deployment and set up monitoring:\n",
        "- Model export\n",
        "- Performance monitoring setup\n",
        "- Alert configuration\n",
        "- Deployment checklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up monitoring thresholds and configurations\n",
        "monitoring_config = {\n",
        "    'performance_thresholds': {\n",
        "        'min_accuracy': 0.55,\n",
        "        'min_sharpe': 1.0,\n",
        "        'max_drawdown': -0.15,\n",
        "        'min_win_rate': 0.52\n",
        "    },\n",
        "    'data_quality_thresholds': {\n",
        "        'max_missing_data': 0.05,\n",
        "        'max_data_delay': 300,  # seconds\n",
        "        'min_confidence': 0.60\n",
        "    },\n",
        "    'alert_levels': {\n",
        "        'warning': 'yellow',\n",
        "        'critical': 'red'\n",
        "    }\n",
        "}\n",
        "\n",
        "class ModelMonitor:\n",
        "    def __init__(self, config, monitoring_config):\n",
        "        self.config = config\n",
        "        self.monitoring_config = monitoring_config\n",
        "        self.metrics_history = []\n",
        "        \n",
        "    def check_model_health(self, predictions, actual):\n",
        "        \"\"\"Check model prediction quality\"\"\"\n",
        "        metrics = {\n",
        "            'timestamp': datetime.now(),\n",
        "            'accuracy': accuracy_score(actual, predictions > 0.5),\n",
        "            'confidence': predictions.mean(),\n",
        "            'prediction_spread': predictions.std()\n",
        "        }\n",
        "        self.metrics_history.append(metrics)\n",
        "        return metrics\n",
        "    \n",
        "    def check_data_quality(self, features):\n",
        "        \"\"\"Check input data quality\"\"\"\n",
        "        return {\n",
        "            'missing_rate': features.isnull().mean().mean(),\n",
        "            'feature_correlation': features.corr().mean().mean(),\n",
        "            'feature_variance': features.var().mean()\n",
        "        }\n",
        "    \n",
        "    def generate_monitoring_report(self):\n",
        "        \"\"\"Generate comprehensive monitoring report\"\"\"\n",
        "        return {\n",
        "            'model_performance': pd.DataFrame(self.metrics_history),\n",
        "            'alerts': self.check_alerts(),\n",
        "            'recommendations': self.generate_recommendations()\n",
        "        }\n",
        "    \n",
        "    def check_alerts(self):\n",
        "        \"\"\"Check for alert conditions\"\"\"\n",
        "        alerts = []\n",
        "        if not self.metrics_history:\n",
        "            return alerts\n",
        "        \n",
        "        latest = self.metrics_history[-1]\n",
        "        thresholds = self.monitoring_config['performance_thresholds']\n",
        "        \n",
        "        if latest['accuracy'] < thresholds['min_accuracy']:\n",
        "            alerts.append({\n",
        "                'level': 'critical',\n",
        "                'message': 'Model accuracy below threshold',\n",
        "                'value': latest['accuracy']\n",
        "            })\n",
        "            \n",
        "        return alerts\n",
        "    \n",
        "    def generate_recommendations(self):\n",
        "        \"\"\"Generate model improvement recommendations\"\"\"\n",
        "        recommendations = []\n",
        "        metrics_df = pd.DataFrame(self.metrics_history)\n",
        "        \n",
        "        if len(metrics_df) > 0:\n",
        "            if metrics_df['accuracy'].tail(10).mean() < 0.6:\n",
        "                recommendations.append(\"Consider retraining model\")\n",
        "            if metrics_df['confidence'].tail(10).mean() < 0.7:\n",
        "                recommendations.append(\"Review feature importance\")\n",
        "                \n",
        "        return recommendations\n",
        "\n",
        "# Create deployment package\n",
        "logger.info(\"Preparing deployment package...\")\n",
        "\n",
        "deployment_package = {\n",
        "    'model': {\n",
        "        'ensemble_path': 'models/final_ensemble.joblib',\n",
        "        'base_models': {\n",
        "            name: f'models/{name}_base.joblib'\n",
        "            for name in base_models\n",
        "        },\n",
        "        'scaler_path': 'models/feature_scaler.joblib'\n",
        "    },\n",
        "    'metadata': {\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'features': selected_features.columns.tolist(),\n",
        "        'model_version': '1.0.0',\n",
        "        'performance_metrics': metrics,\n",
        "        'monitoring_config': monitoring_config\n",
        "    },\n",
        "    'configuration': {\n",
        "        'prediction_threshold': 0.6,\n",
        "        'update_frequency': '24h',\n",
        "        'batch_size': 100\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize monitoring\n",
        "monitor = ModelMonitor(config, monitoring_config)\n",
        "\n",
        "# Generate sample monitoring report\n",
        "test_predictions = ensemble.predict_proba(test_features)[:, 1]\n",
        "monitor.check_model_health(test_predictions, test_target)\n",
        "data_quality = monitor.check_data_quality(test_features)\n",
        "monitoring_report = monitor.generate_monitoring_report()\n",
        "\n",
        "# Plot monitoring metrics\n",
        "if len(monitoring_report['model_performance']) > 0:\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    monitoring_report['model_performance'].set_index('timestamp')['accuracy'].plot()\n",
        "    plt.title('Model Accuracy Over Time')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Save deployment artifacts\n",
        "logger.info(\"Saving deployment artifacts...\")\n",
        "\n",
        "with open('models/deployment_package.json', 'w') as f:\n",
        "    json.dump(deployment_package, f, indent=4)\n",
        "\n",
        "monitoring_report['model_performance'].to_csv('results/monitoring_metrics.csv')\n",
        "\n",
        "# Print deployment checklist\n",
        "print(\"\\nDeployment Checklist:\")\n",
        "print(\"=\" * 50)\n",
        "print(\"1. Model Artifacts:\")\n",
        "print(\"   - Ensemble model saved\")\n",
        "print(\"   - Base models saved\")\n",
        "print(\"   - Feature scaler saved\")\n",
        "print(\"   - Model metadata documented\")\n",
        "\n",
        "print(\"\\n2. Performance Validation:\")\n",
        "print(\"   - Backtesting completed\")\n",
        "print(\"   - Performance metrics calculated\")\n",
        "print(\"   - Risk metrics assessed\")\n",
        "\n",
        "print(\"\\n3. Monitoring Setup:\")\n",
        "print(\"   - Performance thresholds configured\")\n",
        "print(\"   - Data quality checks implemented\")\n",
        "print(\"   - Alert system configured\")\n",
        "\n",
        "print(\"\\n4. Next Steps:\")\n",
        "print(\"   - Set up model API\")\n",
        "print(\"   - Configure data pipeline\")\n",
        "print(\"   - Implement monitoring dashboard\")\n",
        "print(\"   - Establish retraining schedule\")\n",
        "\n",
        "logger.info(\"Pipeline orchestration completed\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPN2eq68mF8YOvowuzGeY3B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
