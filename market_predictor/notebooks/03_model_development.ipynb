{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Predictor: Model Development\n",
    "\n",
    "This notebook focuses on developing and training prediction models:\n",
    "1. Data Preparation\n",
    "2. Base Model Development\n",
    "3. Model Optimization\n",
    "4. Model Validation\n",
    "5. Performance Analysis\n",
    "\n",
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import project modules\n",
    "from src.models import (\n",
    "    ModelFactory,\n",
    "    create_model,\n",
    "    create_ensemble\n",
    ")\n",
    "from src.utils import (\n",
    "    setup_project_logger,\n",
    "    ModelMetrics,\n",
    "    TradingMetrics\n",
    ")\n",
    "from config import Config, load_validated_config\n",
    "\n",
    "# Import sklearn modules\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_project_logger('model_development')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "Load the engineered features and prepare training, validation, and test sets:\n",
    "- Load processed features\n",
    "- Create target variables\n",
    "- Split data into training sets\n",
    "- Scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and data\n",
    "config = load_validated_config('config/parameters.yaml')\n",
    "\n",
    "# Load features\n",
    "features_df = pd.read_parquet('data/features/selected_features.parquet')\n",
    "feature_importance = pd.read_csv('data/features/feature_importance.csv')\n",
    "\n",
    "# Create target variable (next day return direction)\n",
    "returns = features_df['Returns'].shift(-1)  # Next day returns\n",
    "target = np.where(returns > 0, 1, 0)  # 1 for positive returns, 0 for negative\n",
    "target = pd.Series(target[:-1], index=returns.index[:-1])  # Remove last row (NaN)\n",
    "\n",
    "# Prepare features (remove target variable if present and align with target)\n",
    "if 'Returns' in features_df.columns:\n",
    "    features_df = features_df.drop('Returns', axis=1)\n",
    "features_df = features_df.loc[target.index]\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_end = config.data.validation_start\n",
    "val_end = config.data.test_start\n",
    "\n",
    "train_features = features_df[:train_end]\n",
    "train_target = target[:train_end]\n",
    "\n",
    "val_features = features_df[train_end:val_end]\n",
    "val_target = target[train_end:val_end]\n",
    "\n",
    "test_features = features_df[val_end:]\n",
    "test_target = target[val_end:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_features),\n",
    "    columns=train_features.columns,\n",
    "    index=train_features.index\n",
    ")\n",
    "\n",
    "val_scaled = pd.DataFrame(\n",
    "    scaler.transform(val_features),\n",
    "    columns=val_features.columns,\n",
    "    index=val_features.index\n",
    ")\n",
    "\n",
    "test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_features),\n",
    "    columns=test_features.columns,\n",
    "    index=test_features.index\n",
    ")\n",
    "\n",
    "# Print dataset shapes\n",
    "print(\"\\nDataset Shapes:\")\n",
    "print(f\"Training set: {train_scaled.shape}\")\n",
    "print(f\"Validation set: {val_scaled.shape}\")\n",
    "print(f\"Test set: {test_scaled.shape}\")\n",
    "\n",
    "# Plot target distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "target.value_counts(normalize=True).plot(kind='bar')\n",
    "plt.title('Target Class Distribution')\n",
    "plt.xlabel('Return Direction')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Log data preparation completion\n",
    "logger.info('Data preparation completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Base Model Development\n",
    "\n",
    "Develop and evaluate individual base models:\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "Compare their performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model factory\n",
    "model_factory = ModelFactory(config)\n",
    "\n",
    "# Define base models to try\n",
    "base_models = {\n",
    "    'random_forest': {\n",
    "        'model_type': 'random_forest',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            'min_samples_split': 2,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model_type': 'xgboost',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'model_type': 'lightgbm',\n",
    "        'params': {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate base models\n",
    "model_results = {}\n",
    "model_predictions = {}\n",
    "\n",
    "for name, model_config in base_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = model_factory.create_model(\n",
    "        model_config['model_type'],\n",
    "        model_config['params']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.train(train_scaled, train_target, val_scaled, val_target)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(train_scaled)\n",
    "    val_pred = model.predict(val_scaled)\n",
    "    \n",
    "    # Store predictions\n",
    "    model_predictions[name] = {\n",
    "        'train': train_pred,\n",
    "        'val': val_pred\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = ModelMetrics.classification_metrics(train_target, train_pred)\n",
    "    val_metrics = ModelMetrics.classification_metrics(val_target, val_pred)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'train_metrics': train_metrics,\n",
    "        'val_metrics': val_metrics\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{name.upper()} Results:\")\n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {train_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {train_metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {train_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    print(f\"Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "# Plot comparison of model performances\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    for metric in metrics:\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Metric': metric,\n",
    "            'Train': results['train_metrics'][metric],\n",
    "            'Validation': results['val_metrics'][metric]\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
    "    \n",
    "    x = np.arange(len(base_models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, metric_data['Train'], width, label='Train')\n",
    "    plt.bar(x + width/2, metric_data['Validation'], width, label='Validation')\n",
    "    \n",
    "    plt.title(f'{metric.capitalize()} Comparison')\n",
    "    plt.xticks(x, base_models.keys())\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Log model development completion\n",
    "logger.info('Base model development completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Optimization\n",
    "\n",
    "Optimize the best performing base model:\n",
    "- Hyperparameter tuning\n",
    "- Cross-validation analysis\n",
    "- Learning curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from scipy.stats import randint, uniform\n",
    "import joblib\n",
    "\n",
    "# Select best performing model from base models\n",
    "best_model_name = max(model_results, \n",
    "                     key=lambda x: model_results[x]['val_metrics']['f1'])\n",
    "print(f\"Optimizing {best_model_name} model...\")\n",
    "\n",
    "# Define parameter search spaces for each model type\n",
    "param_spaces = {\n",
    "    'random_forest': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(3, 15),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10)\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4)\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'num_leaves': randint(20, 100),\n",
    "        'subsample': uniform(0.6, 0.4)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Setup time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Create and run randomized search\n",
    "model = model_factory.create_model(best_model_name)\n",
    "search = RandomizedSearchCV(\n",
    "    model.model,\n",
    "    param_spaces[best_model_name],\n",
    "    n_iter=50,\n",
    "    cv=tscv,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the randomized search\n",
    "search.fit(train_scaled, train_target)\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(search.best_params_)\n",
    "print(f\"\\nBest cross-validation score: {search.best_score_:.4f}\")\n",
    "\n",
    "# Create model with best parameters\n",
    "optimized_model = model_factory.create_model(\n",
    "    best_model_name,\n",
    "    search.best_params_\n",
    ")\n",
    "\n",
    "# Train and evaluate optimized model\n",
    "optimized_model.train(train_scaled, train_target, val_scaled, val_target)\n",
    "val_pred_opt = optimized_model.predict(val_scaled)\n",
    "\n",
    "# Calculate and print metrics\n",
    "val_metrics_opt = ModelMetrics.classification_metrics(val_target, val_pred_opt)\n",
    "\n",
    "print(\"\\nOptimized Model Validation Metrics:\")\n",
    "print(f\"Accuracy: {val_metrics_opt['accuracy']:.4f}\")\n",
    "print(f\"Precision: {val_metrics_opt['precision']:.4f}\")\n",
    "print(f\"Recall: {val_metrics_opt['recall']:.4f}\")\n",
    "print(f\"F1: {val_metrics_opt['f1']:.4f}\")\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_learning_curves(model, X, y, cv):\n",
    "    train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X, y,\n",
    "        train_sizes=train_sizes,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.plot(train_sizes, val_mean, label='Cross-validation score')\n",
    "    \n",
    "    plt.fill_between(train_sizes, \n",
    "                     train_mean - train_std,\n",
    "                     train_mean + train_std, \n",
    "                     alpha=0.1)\n",
    "    plt.fill_between(train_sizes, \n",
    "                     val_mean - val_std,\n",
    "                     val_mean + val_std, \n",
    "                     alpha=0.1)\n",
    "    \n",
    "    plt.xlabel('Training Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for optimized model\n",
    "plot_learning_curves(\n",
    "    optimized_model.model,\n",
    "    train_scaled,\n",
    "    train_target,\n",
    "    tscv\n",
    ")\n",
    "\n",
    "# Save optimized model\n",
    "joblib.dump(optimized_model, 'models/optimized_model.joblib')\n",
    "logger.info('Model optimization completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Validation and Performance Analysis\n",
    "\n",
    "Comprehensive validation of the optimized model:\n",
    "- Performance metrics\n",
    "- Trading metrics\n",
    "- Error analysis\n",
    "- Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized model\n",
    "optimized_model = joblib.load('models/optimized_model.joblib')\n",
    "\n",
    "# Generate predictions for all datasets\n",
    "predictions = {\n",
    "    'train': optimized_model.predict(train_scaled),\n",
    "    'val': optimized_model.predict(val_scaled),\n",
    "    'test': optimized_model.predict(test_scaled)\n",
    "}\n",
    "\n",
    "probabilities = {\n",
    "    'train': optimized_model.predict_proba(train_scaled),\n",
    "    'val': optimized_model.predict_proba(val_scaled),\n",
    "    'test': optimized_model.predict_proba(test_scaled)\n",
    "}\n",
    "\n",
    "# Calculate performance metrics for each dataset\n",
    "datasets = {\n",
    "    'train': (train_target, train_scaled),\n",
    "    'val': (val_target, val_scaled),\n",
    "    'test': (test_target, test_scaled)\n",
    "}\n",
    "\n",
    "performance_metrics = {}\n",
    "\n",
    "for dataset_name, (y_true, X) in datasets.items():\n",
    "    y_pred = predictions[dataset_name]\n",
    "    y_prob = probabilities[dataset_name]\n",
    "    \n",
    "    metrics = {\n",
    "        'classification': ModelMetrics.classification_metrics(y_true, y_pred, y_prob),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "        'classification_report': classification_report(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    performance_metrics[dataset_name] = metrics\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for idx, (dataset_name, metrics) in enumerate(performance_metrics.items()):\n",
    "    sns.heatmap(metrics['confusion_matrix'], \n",
    "                annot=True, \n",
    "                fmt='d',\n",
    "                ax=axes[idx])\n",
    "    axes[idx].set_title(f'{dataset_name.capitalize()} Confusion Matrix')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate trading metrics\n",
    "def calculate_trading_metrics(y_true, y_pred, returns):\n",
    "    # Create trading strategy returns (long only when predicted 1)\n",
    "    strategy_returns = returns[y_pred == 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'total_return': strategy_returns.sum(),\n",
    "        'sharpe_ratio': TradingMetrics.calculate_sharpe_ratio(strategy_returns),\n",
    "        'max_drawdown': TradingMetrics.calculate_max_drawdown(\n",
    "            (1 + strategy_returns).cumprod()\n",
    "        ),\n",
    "        'win_rate': TradingMetrics.calculate_win_rate(strategy_returns)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "trading_metrics = {}\n",
    "returns_series = pd.Series(returns, index=features_df.index)\n",
    "\n",
    "for dataset_name, (y_true, _) in datasets.items():\n",
    "    y_pred = predictions[dataset_name]\n",
    "    dataset_returns = returns_series[y_true.index]\n",
    "    \n",
    "    trading_metrics[dataset_name] = calculate_trading_metrics(\n",
    "        y_true, y_pred, dataset_returns\n",
    "    )\n",
    "\n",
    "# Plot trading metrics comparison\n",
    "metrics_df = pd.DataFrame(trading_metrics).T\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_df.plot(kind='bar')\n",
    "plt.title('Trading Metrics Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance analysis\n",
    "if hasattr(optimized_model.model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': train_scaled.columns,\n",
    "        'importance': optimized_model.model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title('Top 20 Most Important Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save validation results\n",
    "validation_results = {\n",
    "    'performance_metrics': performance_metrics,\n",
    "    'trading_metrics': trading_metrics,\n",
    "    'feature_importance': feature_importance.to_dict() if 'feature_importance' in locals() else None\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('models/validation_results.json', 'w') as f:\n",
    "    json.dump(validation_results, f, indent=4)\n",
    "\n",
    "logger.info('Model validation completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Deployment Preparation and Next Steps\n",
    "\n",
    "Prepare the model for deployment and summarize findings:\n",
    "- Model serialization\n",
    "- Performance summary\n",
    "- Implementation considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model artifacts and metadata\n",
    "model_artifacts = {\n",
    "    'model_path': 'models/optimized_model.joblib',\n",
    "    'scaler_path': 'models/feature_scaler.joblib',\n",
    "    'features': train_scaled.columns.tolist(),\n",
    "    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'performance_summary': {\n",
    "        dataset: {\n",
    "            'accuracy': metrics['classification']['accuracy'],\n",
    "            'f1': metrics['classification']['f1'],\n",
    "            'precision': metrics['classification']['precision'],\n",
    "            'recall': metrics['classification']['recall']\n",
    "        }\n",
    "        for dataset, metrics in performance_metrics.items()\n",
    "    },\n",
    "    'trading_metrics': trading_metrics,\n",
    "    'model_parameters': optimized_model.get_params(),\n",
    "    'feature_importance': feature_importance.to_dict() if 'feature_importance' in locals() else None\n",
    "}\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'models/feature_scaler.joblib')\n",
    "\n",
    "# Save model metadata\n",
    "with open('models/model_metadata.json', 'w') as f:\n",
    "    json.dump(model_artifacts, f, indent=4)\n",
    "\n",
    "# Create performance summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for dataset in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{dataset.upper()} SET METRICS:\")\n",
    "    print(\"-\" * 20)\n",
    "    metrics = performance_metrics[dataset]['classification']\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    \n",
    "    t_metrics = trading_metrics[dataset]\n",
    "    print(f\"\\nTrading Metrics:\")\n",
    "    print(f\"Total Return: {t_metrics['total_return']:.4f}\")\n",
    "    print(f\"Sharpe Ratio: {t_metrics['sharpe_ratio']:.4f}\")\n",
    "    print(f\"Max Drawdown: {t_metrics['max_drawdown']:.4f}\")\n",
    "    print(f\"Win Rate: {t_metrics['win_rate']:.4f}\")\n",
    "\n",
    "logger.info('Model preparation for deployment completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Model Implementation:\n",
    "   - Set up real-time data pipeline\n",
    "   - Implement prediction scheduling\n",
    "   - Set up monitoring and alerts\n",
    "\n",
    "2. Further Improvements:\n",
    "   - Feature engineering refinements\n",
    "   - Ensemble model development\n",
    "   - Risk management implementation\n",
    "\n",
    "3. Production Considerations:\n",
    "   - Model monitoring setup\n",
    "   - Performance tracking\n",
    "   - Regular retraining schedule\n",
    "\n",
    "Key Files Generated:\n",
    "- `models/optimized_model.joblib`: Trained model\n",
    "- `models/feature_scaler.joblib`: Feature scaler\n",
    "- `models/model_metadata.json`: Model metadata and performance\n",
    "- `models/validation_results.json`: Detailed validation results\n",
    "\n",
    "Proceed to `04_ensemble_training.ipynb` for ensemble model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
